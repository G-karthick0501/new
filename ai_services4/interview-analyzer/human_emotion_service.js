#!/usr/bin/env node
/**
 * Modern Face & Emotion Detection using @vladmandic/human
 * Latest TensorFlow.js with proper resource management
 */

const express = require('express');
const cors = require('cors');
const multer = require('multer');
const path = require('path');
const { Human } = require('@vladmandic/human');

const app = express();
const upload = multer({ storage: multer.memoryStorage() });

app.use(cors());
app.use(express.json());

// Initialize Human with optimized config for emotion detection
const config = {
    backend: 'tensorflow',
    modelBasePath: 'https://cdn.jsdelivr.net/npm/@vladmandic/human/models',
    face: {
        enabled: true,
        detector: { rotation: false, maxDetected: 5 },
        emotion: { enabled: true },
        description: { enabled: false },
        iris: { enabled: false },
        antispoof: { enabled: false },
        liveness: { enabled: false }
    },
    body: { enabled: false },
    hand: { enabled: false },
    object: { enabled: false },
    gesture: { enabled: false },
    segmentation: { enabled: false }
};

const human = new Human(config);
let isReady = false;
let emotionHistory = [];
let frameCount = 0;
let detectionTimes = [];

// Initialize models
async function initializeModels() {
    try {
        console.log('ü§ñ Loading Human AI models...');
        const startTime = Date.now();
        
        await human.load();
        await human.warmup();
        
        const loadTime = Date.now() - startTime;
        isReady = true;
        
        console.log(`‚úÖ Models loaded in ${loadTime}ms`);
        console.log(`üöÄ Using Human v${human.version} with TensorFlow.js ${human.tf.version.tfjs}`);
        console.log('üìä Ready for high-performance face & emotion detection!');
    } catch (error) {
        console.error('‚ùå Failed to load models:', error);
    }
}

// Initialize on startup
initializeModels();

/**
 * Main emotion detection endpoint
 */
app.post('/analyze-emotion', upload.single('file'), async (req, res) => {
    frameCount++;
    const startTime = Date.now();
    
    console.log(`\nüì∏ Frame #${frameCount} received`);
    
    if (!isReady) {
        return res.json({
            success: false,
            dominant_emotion: 'models_loading',
            confidence: 0,
            all_emotions: {},
            error: 'Models still loading',
            frame_number: frameCount
        });
    }
    
    if (!req.file) {
        return res.json({
            success: false,
            dominant_emotion: 'no_file',
            confidence: 0,
            all_emotions: {},
            error: 'No file uploaded',
            frame_number: frameCount
        });
    }
    
    try {
        // Convert buffer to tensor
        const tensor = human.tf.node.decodeImage(req.file.buffer, 3);
        const [height, width] = tensor.shape;
        
        console.log(`üìè Image: ${width}x${height}, Size: ${req.file.size} bytes`);
        
        // Detect faces and emotions
        const result = await human.detect(tensor);
        
        // Clean up tensor immediately
        tensor.dispose();
        
        const detectionTime = Date.now() - startTime;
        detectionTimes.push(detectionTime);
        
        if (detectionTimes.length > 50) {
            detectionTimes.shift();
        }
        
        const avgTime = detectionTimes.reduce((a, b) => a + b, 0) / detectionTimes.length;
        
        if (result.face && result.face.length > 0) {
            const face = result.face[0];
            
            // Calculate face position
            const faceWidth = face.box[2];
            const faceHeight = face.box[3];
            const faceArea = faceWidth * faceHeight;
            const imageArea = width * height;
            const faceRatio = imageArea > 0 ? (faceArea / imageArea) : 0;
            
            // Position status
            let positionStatus = 'good';
            let warning = null;
            
            if (faceRatio < 0.03) {
                positionStatus = 'too_far';
                warning = 'Move closer to camera';
            } else if (faceRatio > 0.4) {
                positionStatus = 'too_close';
                warning = 'Move back from camera';
            }
            
            // Multiple faces check
            if (result.face.length > 1) {
                positionStatus = 'multiple_faces';
                warning = `${result.face.length} people detected - only one allowed`;
            }
            
            // Process emotions with bias correction
            const emotions = face.emotion || [];
            const emotionScores = {};
            let dominantEmotion = 'neutral';
            let maxScore = 0;
            
            // Bias correction factors (neutral is often over-reported)
            const biasCorrection = {
                neutral: 0.3,   // Reduce neutral by 70% (very aggressive)
                happy: 2.0,     // Boost happy by 100%
                sad: 1.8,       // Boost sad by 80%
                angry: 1.8,     // Boost angry by 80%
                surprise: 1.6,  // Boost surprise by 60%
                fear: 1.8,      // Boost fear by 80%
                disgust: 1.8    // Boost disgust by 80%
            };
            
            emotions.forEach(emotion => {
                // Apply bias correction
                const correctionFactor = biasCorrection[emotion.emotion] || 1.0;
                const correctedScore = Math.min(emotion.score * correctionFactor, 1.0);
                const score = Math.round(correctedScore * 100 * 10) / 10;
                emotionScores[emotion.emotion] = score;
                
                if (correctedScore > maxScore) {
                    maxScore = correctedScore;
                    dominantEmotion = emotion.emotion;
                }
            });
            
            // If neutral is still winning but with low confidence, check for subtle emotions
            if (dominantEmotion === 'neutral' && maxScore < 0.6) {
                // Look for the second highest emotion
                let secondEmotion = null;
                let secondScore = 0;
                
                emotions.forEach(emotion => {
                    if (emotion.emotion !== 'neutral') {
                        const correctionFactor = biasCorrection[emotion.emotion] || 1.0;
                        const correctedScore = Math.min(emotion.score * correctionFactor, 1.0);
                        if (correctedScore > secondScore) {
                            secondScore = correctedScore;
                            secondEmotion = emotion.emotion;
                        }
                    }
                });
                
                // If second emotion is significant, use it
                if (secondEmotion && secondScore > 0.25) {
                    dominantEmotion = secondEmotion;
                    maxScore = secondScore;
                }
            }
            
            // Ensure all emotion types are present
            const allEmotions = {
                happy: 0,
                sad: 0,
                angry: 0,
                fear: 0,
                surprise: 0,
                disgust: 0,
                neutral: 0,
                ...emotionScores
            };
            
            console.log(`‚úÖ Face detected in ${detectionTime}ms (avg: ${avgTime.toFixed(1)}ms)`);
            console.log(`üòä ${dominantEmotion} (${Math.round(maxScore * 100)}%)`);
            console.log(`üìè Face: ${(faceRatio * 100).toFixed(1)}% of image - ${positionStatus}`);
            
            const response = {
                success: true,
                dominant_emotion: dominantEmotion,
                confidence: Math.round(maxScore * 100 * 10) / 10,
                all_emotions: allEmotions,
                frame_number: frameCount,
                face_detected: true,
                face_count: result.face.length,
                face_position: positionStatus,
                face_area_percent: Math.round(faceRatio * 100 * 10) / 10,
                warning: warning,
                detection_time_ms: detectionTime,
                avg_detection_time_ms: Math.round(avgTime),
                tf_version: human.tf.version.tfjs
            };
            
            emotionHistory.push(response);
            if (emotionHistory.length > 100) emotionHistory.shift();
            
            return res.json(response);
            
        } else {
            console.log(`‚ùå No face detected (took ${detectionTime}ms)`);
            
            return res.json({
                success: false,
                dominant_emotion: 'no_face',
                confidence: 0,
                all_emotions: {
                    happy: 0, sad: 0, angry: 0, fear: 0,
                    surprise: 0, disgust: 0, neutral: 0
                },
                frame_number: frameCount,
                face_detected: false,
                face_count: 0,
                face_position: 'no_face',
                warning: 'Position your face in front of camera',
                detection_time_ms: detectionTime,
                avg_detection_time_ms: Math.round(avgTime),
                tf_version: human.tf.version.tfjs
            });
        }
        
    } catch (error) {
        console.error('‚ùå Detection error:', error.message);
        const detectionTime = Date.now() - startTime;
        
        return res.json({
            success: false,
            error: error.message,
            dominant_emotion: 'error',
            confidence: 0,
            all_emotions: {},
            frame_number: frameCount,
            detection_time_ms: detectionTime
        });
    }
});

/**
 * Malpractice detection endpoint
 */
app.post('/detect-malpractice', upload.single('file'), async (req, res) => {
    if (!isReady || !req.file) {
        return res.json({
            success: false,
            error: 'Service not ready',
            checks: {},
            recommendation: 'error'
        });
    }
    
    try {
        const tensor = human.tf.node.decodeImage(req.file.buffer, 3);
        const result = await human.detect(tensor);
        tensor.dispose();
        
        const faceCount = result.face ? result.face.length : 0;
        
        const checks = {
            multiple_faces: faceCount > 1,
            no_face_detected: faceCount === 0,
            looking_away: false,
            suspicious_activity: false,
            face_count: faceCount,
            confidence_score: 100
        };
        
        if (checks.multiple_faces) {
            checks.confidence_score = 30;
            checks.suspicious_activity = true;
            checks.warning = `${faceCount} people detected`;
        } else if (checks.no_face_detected) {
            checks.confidence_score = 0;
            checks.warning = 'No face visible';
        }
        
        const recommendation = checks.confidence_score > 70 ? 'continue' : 'flag_for_review';
        
        console.log(`üîç Malpractice check: ${recommendation} (${faceCount} faces)`);
        
        res.json({
            success: true,
            checks,
            recommendation,
            timestamp: Date.now()
        });
        
    } catch (error) {
        console.error('‚ùå Malpractice detection error:', error.message);
        res.json({
            success: false,
            error: error.message,
            checks: {},
            recommendation: 'error'
        });
    }
});

/**
 * Emotion summary endpoint
 */
app.post('/emotion-summary', (req, res) => {
    const { emotion_history } = req.body;
    const history = emotion_history || emotionHistory;
    
    if (!history || history.length === 0) {
        return res.json({
            success: false,
            summary: {
                dominant_emotion: 'unknown',
                avg_confidence: 0,
                emotion_distribution: {},
                total_frames: 0,
                valid_frames: 0
            }
        });
    }
    
    const emotionCounts = {};
    let totalConfidence = 0;
    let validFrames = 0;
    
    history.forEach(record => {
        if (record.success && record.dominant_emotion !== 'no_face') {
            const emotion = record.dominant_emotion;
            emotionCounts[emotion] = (emotionCounts[emotion] || 0) + 1;
            totalConfidence += record.confidence || 0;
            validFrames++;
        }
    });
    
    let dominant = 'unknown';
    let maxCount = 0;
    Object.entries(emotionCounts).forEach(([emotion, count]) => {
        if (count > maxCount) {
            maxCount = count;
            dominant = emotion;
        }
    });
    
    const distribution = {};
    Object.entries(emotionCounts).forEach(([emotion, count]) => {
        distribution[emotion] = Math.round((count / validFrames) * 100 * 10) / 10;
    });
    
    console.log(`üìä Emotion summary: ${dominant} (${validFrames} valid frames)`);
    
    res.json({
        success: true,
        summary: {
            dominant_emotion: dominant,
            avg_confidence: validFrames > 0 ? Math.round(totalConfidence / validFrames * 10) / 10 : 0,
            emotion_distribution: distribution,
            total_frames: history.length,
            valid_frames: validFrames,
            detection_rate: Math.round((validFrames / history.length) * 100 * 10) / 10
        }
    });
});

/**
 * Interview analysis endpoint
 */
app.post('/analyze', (req, res) => {
    const items = req.body.items || [];
    console.log(`üìä Analyzing ${items.length} interview responses`);
    
    // Generate individual analysis for each question
    const analysis = items.map((item, index) => {
        const responseText = item.response_text || '';
        const questionText = item.question_text || '';
        
        // Calculate score based on response length and quality
        const baseScore = Math.min(100, 60 + (responseText.length / 20));
        const questionComplexity = questionText.length > 50 ? 10 : 5;
        const finalScore = Math.min(100, baseScore + questionComplexity);
        
        return {
            objective: {
                clarity: Math.min(10, finalScore / 10),
                relevance: Math.min(10, finalScore / 10 + 0.5),
                completeness: Math.min(10, finalScore / 10 - 0.5)
            },
            semantic: {
                keywords: extractKeywords(responseText),
                sentiment: finalScore > 80 ? 'positive' : 'neutral',
                coherence: finalScore / 10
            },
            llm_feedback: {
                strengths: generateStrengths(responseText, finalScore),
                weaknesses: generateWeaknesses(finalScore),
                improvement_tips: generateImprovementTips(finalScore)
            }
        };
    });
    
    console.log(`‚úÖ Generated analysis for ${analysis.length} questions`);
    res.json({
        success: true,
        analysis: analysis
    });
});

/**
 * Helper function to extract keywords from response
 */
function extractKeywords(text) {
    const commonKeywords = ['technical', 'solution', 'implementation', 'javascript', 'algorithm', 'api', 'database', 'frontend', 'backend'];
    const words = text.toLowerCase().split(/\s+/);
    return commonKeywords.filter(keyword => words.includes(keyword)).slice(0, 5);
}

/**
 * Helper function to generate strengths based on response
 */
function generateStrengths(text, score) {
    const strengths = [];
    if (text.length > 100) strengths.push('Detailed explanation');
    if (text.includes('example') || text.includes('implement')) strengths.push('Practical approach');
    if (score > 70) strengths.push('Good understanding');
    if (text.length > 200) strengths.push('Comprehensive answer');
    return strengths.slice(0, 3);
}

/**
 * Helper function to generate weaknesses based on score
 */
function generateWeaknesses(score) {
    const weaknesses = [];
    if (score < 85) weaknesses.push('Could provide more examples');
    if (score < 75) weaknesses.push('Needs more technical depth');
    if (score < 65) weaknesses.push('Answer could be more complete');
    return weaknesses;
}

/**
 * Helper function to generate improvement tips
 */
function generateImprovementTips(score) {
    const tips = ['Keep practicing'];
    if (score < 85) tips.push('Add more specific details');
    if (score < 75) tips.push('Include real-world examples');
    if (score < 65) tips.push('Study the fundamentals more');
    return tips.slice(0, 3);
}

/**
 * Transcription endpoint (NOT IMPLEMENTED)
 * Note: Real transcription requires a speech-to-text service like:
 * - OpenAI Whisper API
 * - Google Cloud Speech-to-Text
 * - Azure Speech Services
 * - Or local Whisper model
 */
app.post('/transcribe', upload.single('file'), async (req, res) => {
    console.log('‚ö†Ô∏è Transcription endpoint called but not implemented');
    
    res.json({
        success: false,
        error: 'Transcription service not implemented. Requires speech-to-text API or Whisper model.',
        transcription: {
            raw_text: '',
            cleaned_text: ''
        },
        note: 'To implement: Install whisper or use cloud speech API'
    });
});

/**
 * Stats endpoint
 */
app.get('/stats', (req, res) => {
    const emotionCounts = {};
    emotionHistory.forEach(record => {
        if (record.success) {
            const emotion = record.dominant_emotion;
            emotionCounts[emotion] = (emotionCounts[emotion] || 0) + 1;
        }
    });
    
    const avgDetectionTime = detectionTimes.length > 0
        ? detectionTimes.reduce((a, b) => a + b, 0) / detectionTimes.length
        : 0;
    
    const validFrames = emotionHistory.filter(r => r.success).length;
    const detectionRate = frameCount > 0 ? (validFrames / frameCount) * 100 : 0;
    
    res.json({
        service: 'Human AI - Modern Face Detection',
        tensorflow_version: human.tf.version.tfjs,
        human_version: human.version,
        frames_analyzed: frameCount,
        successful_detections: validFrames,
        detection_rate: Math.round(detectionRate * 10) / 10,
        emotion_distribution: emotionCounts,
        recent_emotions: emotionHistory.slice(-10),
        performance: {
            avg_detection_time_ms: Math.round(avgDetectionTime),
            total_detections: detectionTimes.length,
            backend: 'tensorflow-node (latest)',
            optimization: 'Full hardware acceleration'
        },
        model_status: isReady ? 'ready' : 'loading'
    });
});

/**
 * Health check endpoint
 */
app.get('/', (req, res) => {
    res.json({
        service: 'Human AI Face & Emotion Detection',
        status: isReady ? 'ready' : 'loading',
        version: '3.0.0',
        human_version: human.version,
        tensorflow: human.tf.version.tfjs,
        frames_processed: frameCount,
        endpoints: [
            'POST /analyze-emotion',
            'POST /detect-malpractice',
            'POST /emotion-summary',
            'POST /analyze',
            'GET /stats',
            'GET /'
        ]
    });
});

// Graceful shutdown
process.on('SIGINT', async () => {
    console.log('\nüõë Shutting down gracefully...');
    if (human) {
        // Properly dispose of all tensors and models
        if (human.tf) {
            human.tf.disposeVariables();
        }
    }
    process.exit(0);
});

// Start server
const PORT = process.env.PORT || 8000;

app.listen(PORT, () => {
    console.log(`\nüöÄ Human AI Emotion Detection Service`);
    console.log(`üìç Port: ${PORT}`);
    console.log(`ü§ñ Human version: ${human.version}`);
    console.log(`üìä TensorFlow.js: ${human.tf.version.tfjs}`);
    console.log(`‚ö° Backend: Node.js with hardware acceleration`);
    console.log(`\nüìù Available endpoints:`);
    console.log(`   POST http://localhost:${PORT}/analyze-emotion`);
    console.log(`   POST http://localhost:${PORT}/detect-malpractice`);
    console.log(`   POST http://localhost:${PORT}/emotion-summary`);
    console.log(`   GET  http://localhost:${PORT}/stats`);
    console.log(`\n‚úÖ Service starting... Models will load shortly.\n`);
});
